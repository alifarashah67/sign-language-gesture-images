# -*- coding: utf-8 -*-
"""sign-language-gesture-images.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uoduqvGMLIySjrEBlTM2ijJ1qCvNm2D1
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("ahmedkhanak1995/sign-language-gesture-images-dataset")

print("Path to dataset files:", path)

!cp -r /kaggle/input/sign-language-gesture-images-dataset /content/handsignimages

!pip install mediapipe

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
import mediapipe as mp
mpHands = mp.solutions.hands
hands = mpHands.Hands()
mpDraw = mp.solutions.drawing_utils

def hand_landmarks(image):
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  results = hands.process(image)
  landmarks = []
  if results.multi_hand_landmarks:

    for handLms in results.multi_hand_landmarks: # working with each hand

  #                     print(len(handLms.landmark))
        for id, lm in enumerate(handLms.landmark):
            # h, w, c = image.shape
            # cx, cy = int(lm.x * w), int(lm.y * h)
            landmarks.append([lm.x, lm.y, lm.z])
  return landmarks

def loadxy(main_path):
  path = main_path
  labels = os.listdir(path)
  x = []
  y = []
  for label in labels:
    label_path = os.path.join(path, label)

    for img_name in os.listdir(label_path):
      try:

          img_path = os.path.join(label_path, img_name)
          img = cv2.imread(img_path)
          # print(img.shape)
          landmarks = hand_landmarks(img)
          if landmarks:
            if len(landmarks) == 21:
              x.append(landmarks)
              y.append(label)
      except:
        pass

  return x,y

x, y = loadxy(r"/content/handsignimages/Gesture Image Data")

np.unique(y)

def onehot(y):
  classlist = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C',
       'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',
       'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_']
  y_index = list(map(lambda x: classlist.index(x), y))
  return np.eye(len(classlist))[y_index]

y_onehot = onehot(y)

for i in range (len(x)):
  print(np.shape(x[i]))

x = np.array(x)
y_onehot = np.array(y_onehot)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y_onehot, test_size = 0.2, random_state = 42)

y_train.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input

model = Sequential()
model.add(Input(shape = (21, 3)))
# model.add(Conv2D(32, (3, 3), activation = "relu", padding= "same"))
# model.add(MaxPooling2D((2, 2)))
# model.add(Conv2D(64, (3, 3), activation = "relu", padding= "same"))
# model.add(MaxPooling2D((2, 2)))
# model.add(Conv2D(64, (3, 3), activation = "relu", padding= "same"))
# model.add(MaxPooling2D((2, 2)))
# model.add(Conv2D(64, (3, 3), activation = "relu", padding= "same"))
model.add(Flatten())
model.add(Dense(64, activation = "relu"))
model.add(Dense(37, activation = "softmax"))

model.summary()

model.compile(optimizer = "adam", loss = "categorical_crossentropy", metrics = ["accuracy"])
model.fit(x_train, y_train, epochs = 5, batch_size = 32)

model.evaluate(x_test, y_test)

!pip install mediapipe

import mediapipe as mp

mpHands = mp.solutions.hands
hands = mpHands.Hands()
mpDraw = mp.solutions.drawing_utils

import cv2
import numpy as np
import matplotlib.pyplot as plt

image = cv2.imread("/content/handsignimages/Gesture Image Data/C/1.jpg")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image = cv2.resize(image, (256, 256))
blackimage = np.zeros_like(image)
plt.imshow(image)

results = hands.process(image)
landmarks = []
if results.multi_hand_landmarks:

    for handLms in results.multi_hand_landmarks: # working with each hand
#                     print(len(handLms.landmark))
        for id, lm in enumerate(handLms.landmark):
            # h, w, c = image.shape
            # cx, cy = int(lm.x * w), int(lm.y * h)
            landmarks.append([lm.x, lm.y, lm.z])

            # if id == 20 :
            #     cv2.circle(blackimage, (cx, cy), 25, (255, 0, 255), cv2.FILLED)

        mpDraw.draw_landmarks(blackimage, handLms, mpHands.HAND_CONNECTIONS)
plt.imshow(blackimage)
print(results)

landmarks = np.array(landmarks)
landmarks.shape